{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce I - Indegree and Outdegree\n",
    "Instrukcje\n",
    "The goal of this task is to use the map-reduce paradigm to develop algorithms for calculating some characteristics, which describe the structure of large web graphs. A web graph is a directed graph G = (V,E) in which the nodes (vertices) represent single web pages and directed edges correspond to the hyperlinks between the websites. For any two pages u,v \\in V, v contains a link to u iff there exists an edge (u,v) \\in E.\n",
    "\n",
    "The designed algorithms need to be implemented in Apache Spark. You can use Spark with Scala, Java, Python or R. Simply choose your preferred language. Python enthusiasts can also use PySpark in Google Colab.\n",
    "\n",
    "In this assignment we will use the data representing the \"Stanford web graph\" (pages from Stanford University (stanford.edu); collected in 2002). This data isavailable at http://snap.stanford.edu/data/web-Stanford.html This dataset was collected as a part of research on the analysis of social and information networks, including identifying the clusters and determining their properties. If you are interested in this topic, you can find more information about the dataset, the graph structure, and the results of performed research in a paper by J. Leskovec et al. (see http://arxiv.org/abs/0810.1355)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. \n",
    "Download the dataset and analyze the structure of the file (the most important information are briefly described in the first 4 lines). Starting from line 5, the edges are encoded in the form of Vertex1 Vertex2 This corresponds to the existence of a directed edge from Vertex1 to Vertex2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = spark.read.text(\"web-Stanford.txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------\n",
      " value | # Directed graph (each unordered pair of nodes is saved once): web-Stanford.txt  \n",
      "-RECORD 1---------------------------------------------------------------------------------\n",
      " value | # Stanford web graph from 2002                                                   \n",
      "-RECORD 2---------------------------------------------------------------------------------\n",
      " value | # Nodes: 281903 Edges: 2312497                                                   \n",
      "-RECORD 3---------------------------------------------------------------------------------\n",
      " value | # FromNodeId\tToNodeId                                                            \n",
      "-RECORD 4---------------------------------------------------------------------------------\n",
      " value | 1\t6548                                                                           \n",
      "-RECORD 5---------------------------------------------------------------------------------\n",
      " value | 1\t15409                                                                          \n",
      "-RECORD 6---------------------------------------------------------------------------------\n",
      " value | 6548\t57031                                                                       \n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textFile.limit(10).show(n=7, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='# Directed graph (each unordered pair of nodes is saved once): web-Stanford.txt '),\n",
       " Row(value='# Stanford web graph from 2002'),\n",
       " Row(value='# Nodes: 281903 Edges: 2312497'),\n",
       " Row(value='# FromNodeId\\tToNodeId'),\n",
       " Row(value='1\\t6548'),\n",
       " Row(value='1\\t15409'),\n",
       " Row(value='6548\\t57031'),\n",
       " Row(value='15409\\t13102'),\n",
       " Row(value='2\\t17794'),\n",
       " Row(value='2\\t25202')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read to a dataframe\n",
    "Remove first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|        _c0|\n",
      "+-----------+\n",
      "|     1\t6548|\n",
      "|    1\t15409|\n",
      "| 6548\t57031|\n",
      "|15409\t13102|\n",
      "|    2\t17794|\n",
      "|    2\t25202|\n",
      "|    2\t53625|\n",
      "|    2\t54582|\n",
      "|    2\t64930|\n",
      "|    2\t73764|\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = spark.read.csv(\"web-Stanford.txt.gz\",sep='\\n')\n",
    "df = df.filter(~df._c0.contains('#')) #removing the first 5 rows\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Node1|Node2|\n",
      "+-----+-----+\n",
      "|    1| 6548|\n",
      "|    1|15409|\n",
      "| 6548|57031|\n",
      "|15409|13102|\n",
      "|    2|17794|\n",
      "|    2|25202|\n",
      "|    2|53625|\n",
      "|    2|54582|\n",
      "|    2|64930|\n",
      "|    2|73764|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_col = pyspark.sql.functions.split(df['_c0'], '\\t')\n",
    "df = df.withColumn('Node1', split_col.getItem(0))\n",
    "df = df.withColumn('Node2', split_col.getItem(1))\n",
    "df = df.drop(\"_c0\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe to RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Node1='1', Node2='6548'),\n",
       " Row(Node1='1', Node2='15409'),\n",
       " Row(Node1='6548', Node2='57031'),\n",
       " Row(Node1='15409', Node2='13102'),\n",
       " Row(Node1='2', Node2='17794'),\n",
       " Row(Node1='2', Node2='25202'),\n",
       " Row(Node1='2', Node2='53625'),\n",
       " Row(Node1='2', Node2='54582'),\n",
       " Row(Node1='2', Node2='64930'),\n",
       " Row(Node1='2', Node2='73764')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df.rdd\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is prepared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\n",
    "Solve Problem 36 from the problem set by Dr. M. GÄ™bala. For each vertex v \\in V you need to determine its indegree and outdegree, i.e. the number of incoming and outgoing edges. Design and implement the procedures for mapper and reducer. Also write the procedure for determining the average indegree and outdegree for the graph. Try to make your implementation as efficient as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outdegree\n",
    "\n",
    "Map : (node1,node2) -> (node1,1)\n",
    "\n",
    "Reduce : (node1,1) , (node1,1) -> (node1,1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdegree_pairs = rdd.map(lambda x: (x[0],1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 1),\n",
       " ('1', 1),\n",
       " ('6548', 1),\n",
       " ('15409', 1),\n",
       " ('2', 1),\n",
       " ('2', 1),\n",
       " ('2', 1),\n",
       " ('2', 1),\n",
       " ('2', 1),\n",
       " ('2', 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdegree_pairs.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdegree_counts = outdegree_pairs.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdegree = outdegree_counts.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Outdegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "| 82409|255|\n",
      "| 82868|247|\n",
      "|180611|247|\n",
      "| 16984|247|\n",
      "| 86290|247|\n",
      "|188978|247|\n",
      "| 10699|245|\n",
      "|121634|245|\n",
      "|176419|244|\n",
      "|255711|244|\n",
      "+------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outdegree.orderBy(\"_2\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indegree\n",
    "\n",
    "Map : (node1,node2) -> (node2,1)\n",
    "\n",
    "Reduce : (node2,1) , (node2,1) -> (node2,1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('6548', 1),\n",
       " ('15409', 1),\n",
       " ('57031', 1),\n",
       " ('13102', 1),\n",
       " ('17794', 1),\n",
       " ('25202', 1),\n",
       " ('53625', 1),\n",
       " ('54582', 1),\n",
       " ('64930', 1),\n",
       " ('73764', 1)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indegree_pairs = rdd.map(lambda x: (x[1],1)) \n",
    "indegree_pairs.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|    _1|   _2|\n",
      "+------+-----+\n",
      "|226411|38606|\n",
      "|234704|21920|\n",
      "|105607|19457|\n",
      "|241454|19377|\n",
      "|167295|19003|\n",
      "|198090|18975|\n",
      "| 81435|18970|\n",
      "|214128|18967|\n",
      "| 38342|18958|\n",
      "|245659|18935|\n",
      "+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indegree_counts = indegree_pairs.reduceByKey(lambda a, b: a + b) \n",
    "\n",
    "indegree = indegree_counts.toDF()\n",
    "\n",
    "indegree.orderBy(\"_2\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Indegrees higher than top outdegrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean as _mean, col\n",
    "indegree_avg = indegree.select(_mean(col('_2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|          avg(_2)|\n",
      "+-----------------+\n",
      "|8.840225851338747|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indegree_avg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|          avg(_2)|\n",
      "+-----------------+\n",
      "|8.208173754396924|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outdegree_avg = outdegree.select(_mean(col('_2')))\n",
    "outdegree_avg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average map-reduce\n",
    "\n",
    "Map : (node, count) -> (\"sum\",count)\n",
    "\n",
    "GroupByKey : (\"sum\",count1) , (\"sum\",count2), ... -> (\"sum\", (count1,count2,...))\n",
    "\n",
    "Reduce: (\"sum\",(count1,count2,..))  -> (\"sum\",sum(counts)/len(counts) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "indegree_avg2 = indegree_counts.map(lambda x: (\"mean\",x[1]))\n",
    "indegree_avg2 = indegree_avg2.groupByKey().mapValues(lambda x: sum(x) / len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mean', 8.840225851338747)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indegree_avg2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mean', 8.208173754396924)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdegree_avg2 = outdegree_counts.map(lambda x: (\"mean\",x[1]))\n",
    "outdegree_avg2 = outdegree_avg2.groupByKey().mapValues(lambda x: sum(x) / len(x))\n",
    "outdegree_avg2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same results for Map-Reduce and usual mean function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\n",
    "Write a short paragraph in which you summarize the general idea of algorithms used in 2. Try to estimate how many times each edge / each vertex is processed.\n",
    "\n",
    "The general idea is based on the Map-Reduce paradigm. I have a column of pairs (node1, node2) and by using mapping and reducing I transform the node pairs into count pairs. The design of mapping and reducing is described above. The functions are lazy evaluated so until I call them to show the result they don't perform the calculations actually. Such programmes can be run on a cluster by splitting the tasks on different nodes.\n",
    "\n",
    "Number of processed times for each node in a map and reduce for indegree/outdegree equals indegree/outdegree of the node. The total number of calculations performed is equal to the number of edges present.\n",
    "\n",
    "The average calculation is based on the previusly created degree table. The complexity of the map calculation depends on the number of nodes present in a network.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
